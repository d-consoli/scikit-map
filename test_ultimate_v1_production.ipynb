{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37925c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "os.environ['PROJ_LIB'] = '/opt/conda/share/proj/'\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '96'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '96'\n",
    "#os.environ['OMP_THREAD_LIMIT'] = '96'\n",
    "#os.environ['OMP_NUM_THREADS'] = '96'\n",
    "\n",
    "from datetime import datetime\n",
    "from osgeo import gdal, gdal_array\n",
    "from pathlib import Path\n",
    "from typing import Callable, Iterator, List,  Union\n",
    "import bottleneck as bn\n",
    "import geopandas as gpd\n",
    "import numexpr as ne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import SharedArray as sa\n",
    "import skmap_bindings\n",
    "import tempfile\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import joblib\n",
    "from hummingbird.ml import load\n",
    "import traceback\n",
    "import treelite_runtime\n",
    "\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from itertools import islice\n",
    "\n",
    "ne.set_num_threads(96)\n",
    "\n",
    "gdal_opts = {\n",
    " #'GDAL_HTTP_MULTIRANGE': 'SINGLE_GET',\n",
    " #'GDAL_HTTP_MERGE_CONSECUTIVE_RANGES': 'NO',\n",
    " 'GDAL_HTTP_VERSION': '1.0',\n",
    " #'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',\n",
    " #'VSI_CACHE': 'FALSE',\n",
    " 'CPL_VSIL_CURL_ALLOWED_EXTENSIONS': '.tif',\n",
    " #'GDAL_HTTP_CONNECTTIMEOUT': '320',\n",
    " #'CPL_VSIL_CURL_USE_HEAD': 'NO',\n",
    " #'GDAL_HTTP_TIMEOUT': '320',\n",
    " #'CPL_CURL_GZIP': 'NO'\n",
    "}\n",
    "\n",
    "co = ['TILED=YES', 'BIGTIFF=YES', 'COMPRESS=DEFLATE', 'BLOCKXSIZE=1024', 'BLOCKYSIZE=1024']\n",
    "\n",
    "executor = None\n",
    "\n",
    "def ttprint(*args, **kwargs):\n",
    "  from datetime import datetime\n",
    "  import sys\n",
    "\n",
    "  print(f'[{datetime.now():%H:%M:%S}] ', end='')\n",
    "  print(*args, **kwargs, flush=True)\n",
    "\n",
    "def make_tempdir(basedir='skmap', make_subdir = True):\n",
    "  tempdir = Path(TMP_DIR).joinpath(basedir)\n",
    "  if make_subdir: \n",
    "    name = Path(tempfile.NamedTemporaryFile().name).name\n",
    "    tempdir = tempdir.joinpath(name)\n",
    "  tempdir.mkdir(parents=True, exist_ok=True)\n",
    "  return tempdir\n",
    "\n",
    "def make_tempfile(basedir='skmap', prefix='', suffix='', make_subdir = False):\n",
    "  tempdir = make_tempdir(basedir, make_subdir=make_subdir)\n",
    "  return tempdir.joinpath(\n",
    "    Path(tempfile.NamedTemporaryFile(prefix=prefix, suffix=suffix).name).name\n",
    "  )\n",
    "        \n",
    "def _features(csv_file, years, tile_id, model_name, rfe_fn):\n",
    "  \n",
    "  df_features = pd.read_csv(csv_file,index_col=0)\n",
    "\n",
    "  df_list = []\n",
    "  df_list += [ df_features[(df_features['type'] == 'static')] ]\n",
    "\n",
    "  for year in years:\n",
    "    mask = (df_features['type'] == 'landsat')\n",
    "    df = df_features[mask].copy()\n",
    "    df['path'] = df['path'].apply(lambda p: p.replace('{tile}', tile_id).replace('{year}', str(year)))\n",
    "    df_list += [ df ]\n",
    "\n",
    "  otf_mask = df_features['type'] == 'on-the-fly'\n",
    "  otf_sel = df_features[otf_mask]['name'].apply(lambda f: '_'.join(f.split('_')[0:2])).unique()\n",
    "  for year in years:\n",
    "    df_list += [ df_features[np.logical_and(otf_mask, df_features['name'].str.contains('|'.join(otf_sel)))] ]\n",
    "\n",
    "  df_features = pd.concat(df_list)\n",
    "  df_features = df_features.sort_values(['idx', 'path']).reset_index(drop=True)\n",
    "  df_features['idx'] = df_features.index\n",
    "\n",
    "  matrix_idx = []\n",
    "\n",
    "  for c in pd.read_csv(rfe_fn)['name']:\n",
    "    sel_mask = df_features['name'] == c\n",
    "    idx = list(df_features[sel_mask]['idx'])\n",
    "    if len(idx) == 1:\n",
    "      idx = [ idx[0] for i in range(0,len(years)) ]\n",
    "    matrix_idx.append(idx)\n",
    "\n",
    "  matrix_idx = np.array(matrix_idx)\n",
    "  \n",
    "  return df_features, matrix_idx\n",
    "\n",
    "def _raster_paths(df_features, ftype):\n",
    "\n",
    "  mask = (df_features['type'] == ftype)\n",
    "  ids_list = list(df_features[mask]['idx'])\n",
    "  raster_files = list(df_features[mask]['path'])\n",
    "\n",
    "  return raster_files, ids_list\n",
    "\n",
    "def _get_static_layers_info(df_features, tiles, tile):\n",
    "  \n",
    "  min_x, _, _, max_y = tiles[tiles['TILE'] == tile].iloc[0].geometry.bounds\n",
    "  static_files, static_idx = _raster_paths(df_features, 'static')\n",
    "  \n",
    "  gidal_ds = gdal.Open(static_files[0]) # It is assumed to be the same for all static layers\n",
    "  gt = gidal_ds.GetGeoTransform()\n",
    "  gti = gdal.InvGeoTransform(gt)\n",
    "  x_off_s, y_off_s = gdal.ApplyGeoTransform(gti, min_x, max_y)\n",
    "  x_off_s, y_off_s = int(x_off_s), int(y_off_s)\n",
    "  \n",
    "  return static_files, static_idx, x_off_s, y_off_s\n",
    "\n",
    "def _geom_temperature(df_features, array, n_threads):\n",
    "\n",
    "  elev_idx = list(df_features[df_features['name'].str.contains('dtm.bareearth_ensemble')].index)\n",
    "  lst_min_geo_idx = list(df_features[df_features['name'].str.contains('clm_lst_min.geom.temp')].index)\n",
    "  lst_max_geo_idx = list(df_features[df_features['name'].str.contains('clm_lst_max.geom.temp')].index)\n",
    "\n",
    "  x_off, y_off = (0, 0)\n",
    "  base_landsat = landsat_files[-1]\n",
    "  lon_lat = np.zeros((2, array.shape[1]), dtype=np.float32)\n",
    "  skmap_bindings.getLatLonArray(lon_lat, n_threads, gdal_opts, base_landsat, x_off, y_off, x_size, y_size)\n",
    "  latitude = lon_lat[1,:].copy()\n",
    "\n",
    "  doys = [ datetime.strptime(f'2000-{m}-15', '%Y-%m-%d').timetuple().tm_yday for m in range(1,13) ]\n",
    "  doys_all = sum([ doys for i in range(0, len(years)) ],[])\n",
    "\n",
    "  elevation = array[elev_idx[0],:]\n",
    "\n",
    "  skmap_bindings.computeGeometricTemperature(array, n_threads, latitude, elevation, 0.1, 24.16453, -15.71751, 100., lst_min_geo_idx, doys_all)\n",
    "  skmap_bindings.computeGeometricTemperature(array, n_threads, latitude, elevation, 0.1, 37.03043, -15.43029, 100., lst_max_geo_idx, doys_all)\n",
    "\n",
    "def in_mem_calc(data, df_features, n_threads):\n",
    "    \n",
    "  band_scaling = 0.004\n",
    "  result_scaling = 125.\n",
    "  result_offset = 125.\n",
    "\n",
    "  blue_idx = list(df_features[df_features['name'].str.contains('blue_glad')].index)\n",
    "  red_idx = list(df_features[df_features['name'].str.contains('red_glad')].index)\n",
    "  nir_idx = list(df_features[df_features['name'].str.contains('nir_glad')].index)\n",
    "\n",
    "  swir1_idx = list(df_features[df_features['name'].str.contains('swir1_glad')].index)\n",
    "  swir2_idx = list(df_features[df_features['name'].str.contains('swir2_glad')].index)\n",
    "  bsf_idx = list(df_features[df_features['name'].str.contains('bsf')].index)\n",
    "\n",
    "  ndvi_idx = list(df_features[df_features['name'].str.contains('ndvi_glad')].index)\n",
    "  ndwi_idx = list(df_features[df_features['name'].str.contains('ndwi_glad')].index)\n",
    "  bsi_idx = list(df_features[df_features['name'].str.contains('bsi_glad')].index)\n",
    "  ndti_idx = list(df_features[df_features['name'].str.contains('ndti_glad')].index)\n",
    "  nirv_idx = list(df_features[df_features['name'].str.contains('nirv_glad')].index)\n",
    "  evi_idx = list(df_features[df_features['name'].str.contains('evi_glad')].index)\n",
    "  fapar_idx = list(df_features[df_features['name'].str.contains('fapar_glad')].index)\n",
    "\n",
    "  # NDVI\n",
    "  skmap_bindings.computeNormalizedDifference(data, n_threads,\n",
    "                            nir_idx, red_idx, ndvi_idx,\n",
    "                            band_scaling, band_scaling, result_scaling, result_offset, [0., 250.])\n",
    "  # NDWI\n",
    "  skmap_bindings.computeNormalizedDifference(data, n_threads,\n",
    "                            nir_idx, swir1_idx, ndwi_idx,\n",
    "                            band_scaling, band_scaling, result_scaling, result_offset, [0., 250.])\n",
    "  # BSI\n",
    "  skmap_bindings.computeBsi(data, n_threads,\n",
    "                            swir1_idx, red_idx, nir_idx, blue_idx, bsi_idx,\n",
    "                            band_scaling, band_scaling, band_scaling, band_scaling, result_scaling, result_offset, [0., 250.])\n",
    "  # NDTI\n",
    "  skmap_bindings.computeNormalizedDifference(data, n_threads,\n",
    "                            swir1_idx, swir2_idx, ndti_idx,\n",
    "                            band_scaling, band_scaling, result_scaling, result_offset, [0., 250.])\n",
    "\n",
    "  # NIRV\n",
    "  #expr = '( ( ( ( (nir * 0.004) - (red * 0.004) ) / ( (nir * 0.004) + (red * 0.004) ) ) - 0.08) *  (nir * 0.004) ) * 125 + 125'\n",
    "  #data[nir_idx,:] = ne.evaluate(expr, local_dict={ 'red':data[red_idx,:], 'nir':data[nir_idx,:] }).round()\n",
    "  skmap_bindings.computeNirv(data, n_threads,\n",
    "                            nir_idx, red_idx, nirv_idx,\n",
    "                            band_scaling, band_scaling, result_scaling, result_offset, [0., 250.])\n",
    "  # EVI\n",
    "  #expr = '( 2.5 * ( (nir * 0.004) - (red * 0.004) ) / ( (nir * 0.004) + 6 * (red * 0.004) - 7.5 * (blue * 0.004) + 1) ) * 125 + 125'\n",
    "  #data[evi_idx,:] = ne.evaluate(expr, local_dict={ 'red':data[red_idx,:], 'nir':data[nir_idx,:], 'blue': data[blue_idx,:]  }).round()\n",
    "  skmap_bindings.computeEvi(data, n_threads,\n",
    "                            red_idx, nir_idx, blue_idx, evi_idx,\n",
    "                            band_scaling, band_scaling, band_scaling, result_scaling, result_offset, [0., 250.])\n",
    "\n",
    "  # FAPAR\n",
    "  skmap_bindings.computeFapar(data, n_threads,\n",
    "                            red_idx, nir_idx, fapar_idx,\n",
    "                            band_scaling, band_scaling, result_scaling, result_offset, [0., 250.])\n",
    "  \n",
    "  _geom_temperature(df_features, array, n_threads)\n",
    "\n",
    "def _processed(tile):\n",
    "  url = f'http://192.168.49.30:8333/tmp-gpw/v20240418_cpp2/{tile}/gpw_eml.grass.type_30m_m_20220101_20221231_go_epsg.4326_v20240206.tif'\n",
    "  r = requests.head(url)\n",
    "  return (r.status_code == 200)\n",
    "\n",
    "def get_SWAG_weights(att_env, att_seas, season_size, n_imag):\n",
    "  conv_mat_row = np.zeros((n_imag))\n",
    "  base_func = np.zeros((season_size,))\n",
    "  period_y = season_size/2.0\n",
    "  slope_y = att_seas/10/period_y\n",
    "  for i in np.arange(season_size):\n",
    "      if i <= period_y:\n",
    "          base_func[i] = -slope_y*i\n",
    "      else:\n",
    "          base_func[i] = slope_y*(i-period_y)-att_seas/10\n",
    "  # Compute the envelop to attenuate temporarly far images\n",
    "  env_func = np.zeros((n_imag,))\n",
    "  delta_e = n_imag\n",
    "  slope_e = att_env/10/delta_e\n",
    "  for i in np.arange(delta_e):\n",
    "      env_func[i] = -slope_e*i\n",
    "      conv_mat_row = 10.0**(np.resize(base_func,n_imag) + env_func)\n",
    "  return conv_mat_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "827f84b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:33:14] Processing 1 tiles\n",
      "[11:33:14] Reading tiling system\n",
      "[11:33:20] Tile 039E_20N - Reading static: 2.33 segs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: /vsicurl/http://192.168.49.41:8333/global/lcv/lcv_water.distance_glad.interanual.dynamic.classes_pxl_100m_0..0cm_1999..2021_v2021.tif: This file used to have optimizations in its layout, but those have been, at least partly, invalidated by later changes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:33:21] Tile 039E_20N - Total reading and gap-filling landsat: 1.46 segs\n",
      "[11:33:31] Tile 039E_20N - In memory calc: 9.94 segs\n",
      "[11:33:31] Tile 039E_20N - Reading mask and allocating memory: 0.18 segs\n",
      "[11:33:37] Tile 039E_20N - Transposing data: 5.39 segs\n",
      "[11:33:37] Tile 039E_20N - Masking data: 0.05 segs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opengeohub/.local/lib/python3.8/site-packages/treelite_runtime/warn.py:6: UserWarning: class treelite_runtime.DMatrix is deprecated and scheduled for removal in Treelite 4.0. Please use TL2cgen instead. Consult the migration guide at https://tl2cgen.readthedocs.io/en/latest/treelite-migration.html.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:33:37] Tile 039E_20N - Creating dmatrix: 0.38 segs\n",
      "[11:33:37] Tile 039E_20N - Prediction error \n",
      "[11:33:37] Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12411/2432465188.py\", line 181, in <module>\n",
      "    model = models[model_name]\n",
      "KeyError: 'class_3r'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_tile = 10\n",
    "end_tile = 11\n",
    "server_name = 'landmark'\n",
    "\n",
    "\n",
    "TMP_DIR = tempfile.gettempdir()\n",
    "\n",
    "base_dir = Path('/mnt/tupi/WRI/prod_new_samples')\n",
    "model_dir = Path(f'/mnt/{server_name}/wri_pasture_class/compiled')\n",
    "\n",
    "# Parameters for gap-filling\n",
    "file_ending = '_go_epsg.4326_v20230908.tif'\n",
    "gaia_prefix = '/vsicurl/http://192.168.49.'\n",
    "bands_prefix = ['/seasconv/blue_glad.SeasConv.ard2_m_30m_s_',\n",
    "                '/seasconv/green_glad.SeasConv.ard2_m_30m_s_',\n",
    "                '/seasconv/nir_glad.SeasConv.ard2_m_30m_s_',\n",
    "                '/seasconv/red_glad.SeasConv.ard2_m_30m_s_',\n",
    "                '/seasconv/swir1_glad.SeasConv.ard2_m_30m_s_',\n",
    "                '/seasconv/swir2_glad.SeasConv.ard2_m_30m_s_',\n",
    "                '/seasconv/thermal_glad.SeasConv.ard2_m_30m_s_']\n",
    "SIRCLE_years = range(2000,2023)\n",
    "n_s = len(SIRCLE_years) * 6\n",
    "in_idx = range(n_s)\n",
    "n_seas = int(n_s/len(SIRCLE_years))\n",
    "w_p = (get_SWAG_weights(46, 45, n_seas, n_s)[1:][::-1]).astype(np.float32)\n",
    "w_f = (get_SWAG_weights(46, 45, n_seas, n_s)[1:]).astype(np.float32)\n",
    "n_b = len(bands_prefix)\n",
    "keep_original_values = True\n",
    "version = 'v2'\n",
    "backend = 'Matrix'\n",
    "w_0 = 1.0\n",
    "\n",
    "\n",
    "# Keep the order\n",
    "model_names = [ 'class_3r', 'class_12', 'class_13', 'class_23']\n",
    "class_3r_th = 47\n",
    "models = {}\n",
    "\n",
    "for tc in model_names:\n",
    "  import tl2cgen\n",
    "  fn_model = str(model_dir.joinpath(f'model_{tc}_rf.so'))\n",
    "  # ttprint(f'Reading model {fn_model}')\n",
    "  # #models[tc] = tl2cgen.Predictor(libpath=fn_model, nthread=96)\n",
    "  # models[tc] = treelite_runtime.Predictor(fn_model)\n",
    "\n",
    "month_start = ['0101'\n",
    "           ,'0301'\n",
    "           ,'0501'\n",
    "           ,'0701'\n",
    "           ,'0901'\n",
    "           ,'1101']\n",
    "month_end = ['0228'\n",
    "           ,'0430'\n",
    "           ,'0630'\n",
    "           ,'0831'\n",
    "           ,'1031'\n",
    "           ,'1231']\n",
    "\n",
    "mask_prefix = 'http://192.168.1.30:8333/gpw/landmask'\n",
    "tiles_fn = '/mnt/slurm/jobs/wri_pasture_class/gpw_tiles.gpkg' #str(base_dir.joinpath('/mnt/slurm/jobs/wri_pasture_class/gpw_tiles.gpkg'))\n",
    "ids_fn = '/mnt/slurm/jobs/wri_pasture_class/gpw_pasture.class_ids.csv' #str(base_dir.joinpath('gpw_pasture.class_ids_lasts.csv'))\n",
    "features_fn = str(model_dir.joinpath('features_hierarchical.csv'))\n",
    "rfe_fn = str(model_dir.joinpath('rfe_all_features_hierarchical.csv'))\n",
    "\n",
    "\n",
    "\n",
    "years = range(2000,2022 + 1, 2)\n",
    "x_size, y_size = (4004, 4004)\n",
    "n_threads = 96\n",
    "n_classes = 3\n",
    "s3_prefix = 'tmp-gpw/v20240418_cpp2'\n",
    "\n",
    "subnet = '192.168.49'\n",
    "hosts = [ f'{subnet}.{i}:8333' for i in range(30,43) ]\n",
    "\n",
    "tiles_id = pd.read_csv(ids_fn)['TILE'][start_tile:end_tile]\n",
    "\n",
    "ttprint(f\"Processing {len(tiles_id)} tiles\")\n",
    "\n",
    "ttprint(\"Reading tiling system\")\n",
    "tiles = gpd.read_file(tiles_fn)\n",
    "\n",
    "for tile_id in tiles_id:\n",
    "\n",
    "  try:\n",
    "    if _processed(tile_id):\n",
    "      ttprint(f\"Tile {tile_id} is processed. Ignoring it.\")\n",
    "      continue\n",
    "\n",
    "    minx, miny, maxx, maxy = tiles[tiles['TILE'] == tile_id].iloc[0].geometry.bounds\n",
    "\n",
    "    df_features, matrix_idx = _features(features_fn, years, tile_id, model_names[0], rfe_fn)\n",
    "\n",
    "    bands_list = [1,]\n",
    "    n_rasters = df_features.shape[0]\n",
    "    \n",
    "    shape = (n_rasters, x_size * y_size)\n",
    "    array = np.empty(shape, dtype=np.float32)\n",
    "\n",
    "    landsat_files, landsat_idx = _raster_paths(df_features, 'landsat')\n",
    "    static_files, static_idx, x_off_s, y_off_s = _get_static_layers_info(df_features, tiles, tile_id)\n",
    "\n",
    "    start = time.time()\n",
    "    skmap_bindings.readData(array, n_threads, static_files, static_idx, x_off_s, y_off_s, x_size, y_size, bands_list, gdal_opts)\n",
    "    ttprint(f\"Tile {tile_id} - Reading static: {(time.time() - start):.2f} segs\")\n",
    "\n",
    "    start = time.time()\n",
    "    x_off_d, y_off_d = (0, 0)\n",
    "    landsat_files = [str(r).replace(f\"{subnet}.30\", f\"{subnet}.{30 + int.from_bytes(Path(r).stem.encode(), 'little') % len(hosts)}\") for r in landsat_files]    \n",
    "    landsat_data_t = np.empty((x_size*y_size*n_b, n_s), dtype=np.float32)\n",
    "    landsat_metadata = []\n",
    "    for b in range(n_b):\n",
    "      band_files = []\n",
    "      for year in SIRCLE_years:\n",
    "        for bimonth in range(6):\n",
    "          band_files.append(f'{gaia_prefix}{30+((bimonth+6*year)%13)}:8333/prod-landsat-ard2/{tile_id}{bands_prefix[b]}{year}{month_start[bimonth]}_{year}{month_end[bimonth]}{file_ending}')\n",
    "      band_data = np.empty((n_s, x_size*y_size), dtype=np.float32)\n",
    "      landsat_metadata.append(band_files)\n",
    "      skmap_bindings.readData(band_data, n_threads, band_files, in_idx, x_off_d, y_off_d, x_size, y_size, bands_list, gdal_opts, 255., np.nan)\n",
    "      \n",
    "      band_data_t = np.empty((band_data.shape[1], band_data.shape[0]), dtype=np.float32)\n",
    "      skmap_bindings.transposeArray(band_data, n_threads, band_data_t)\n",
    "      skmap_bindings.applySircle(band_data_t, n_threads, landsat_data_t, x_size*y_size*b, w_0, w_p, w_f, keep_original_values, version, backend)\n",
    "    perm_matrix = np.empty((len(landsat_files),3), dtype=np.uintc)\n",
    "    for i, landsat_file in enumerate(landsat_files):\n",
    "        perm_matrix[i,0] = landsat_idx[i]\n",
    "        flag_landsat = True\n",
    "        for j in range(n_b):\n",
    "            for k in range(n_s):\n",
    "                if landsat_metadata[j][k].split('/')[-1] == landsat_file.split('/')[-1]:\n",
    "                    perm_matrix[i,1] = j\n",
    "                    perm_matrix[i,2] = k\n",
    "                    flag_landsat = False\n",
    "        if flag_landsat:\n",
    "            print('Error: Landsat file not found in metadata')\n",
    "            sys.exit(1)\n",
    "    \n",
    "    skmap_bindings.transposeReorderArray(landsat_data_t, n_threads, array, perm_matrix)\n",
    "    ttprint(f\"Tile {tile_id} - Total reading and gap-filling landsat: {(time.time() - start):.2f} segs\")\n",
    "\n",
    "    start = time.time()\n",
    "    in_mem_calc(array, df_features, n_threads)\n",
    "    ttprint(f\"Tile {tile_id} - In memory calc: {(time.time() - start):.2f} segs\")\n",
    "    \n",
    "    start = time.time()\n",
    "    mask_file = f'{mask_prefix}/{tile_id}.tif'\n",
    "    mask = np.zeros((1,x_size * y_size), dtype=np.float32)\n",
    "    skmap_bindings.readData(mask, n_threads, [mask_file,], [0,], x_off_d, x_off_d, x_size, y_size, [1,], gdal_opts)\n",
    "\n",
    "    n_data = int(np.sum(mask)) * len(years)\n",
    "    selected_pix = np.arange(0, x_size * y_size)[mask[0,:] == 1]\n",
    "    selected_rows = np.concatenate([ selected_pix + (x_size * y_size) * i for i in range(0,len(years)) ]).tolist()\n",
    "\n",
    "    shape = (n_data, len(model_names) + 1)\n",
    "    out = np.empty(shape, dtype=np.float32)\n",
    "    \n",
    "    ttprint(f\"Tile {tile_id} - Reading mask and allocating memory: {(time.time() - start):.2f} segs\")\n",
    "\n",
    "    start = time.time()\n",
    "    n_features = len(df_features[df_features[model_names[0]] > -1]['name'].unique()) #df_features[model_name].max() + 1\n",
    "    n_pix = len(years) * x_size * y_size\n",
    "    array_mem_t = np.empty((n_pix, n_features), dtype=np.float32)\n",
    "    array_mem = np.empty((n_features, n_pix), dtype=np.float32)\n",
    "\n",
    "    skmap_bindings.reorderArray(array, n_threads, array_mem, matrix_idx)\n",
    "    skmap_bindings.transposeArray(array_mem, n_threads, array_mem_t)\n",
    "    ttprint(f\"Tile {tile_id} - Transposing data: {(time.time() - start):.2f} segs\")\n",
    "\n",
    "    start = time.time()\n",
    "    array_t = np.empty((n_data, n_features), dtype=np.float32)\n",
    "    skmap_bindings.selArrayRows(array_mem_t, n_threads, array_t, selected_rows)\n",
    "    ttprint(f\"Tile {tile_id} - Masking data: {(time.time() - start):.2f} segs\")\n",
    "\n",
    "    start = time.time()\n",
    "    dmatrix = treelite_runtime.DMatrix(array_t)\n",
    "    #dmatrix = tl2cgen.DMatrix(array_t)\n",
    "    ttprint(f\"Tile {tile_id} - Creating dmatrix: {(time.time() - start):.2f} segs\")  \n",
    "\n",
    "    for mi, model_name in zip(range(0,len(model_names)), model_names):\n",
    "\n",
    "      model = models[model_name]    \n",
    "\n",
    "      start = time.time()\n",
    "      proba = model.predict(dmatrix)\n",
    "\n",
    "      if model_name == 'class_3r':\n",
    "        out[:,mi] = proba[:] * 100\n",
    "      else:\n",
    "        out[:,mi] = (1 - proba[:]) * 100\n",
    "\n",
    "      ttprint(f\"Tile {tile_id} - Running model {model_name}: {(time.time() - start):.2f} segs\")\n",
    "\n",
    "    start = time.time()\n",
    "    hir_idx = len(model_names)\n",
    "    out[:,hir_idx] = out[:,1]\n",
    "    out[:,hir_idx][(out[:,0] < class_3r_th)] = 255\n",
    "    ttprint(f\"Tile {tile_id} - Integrating class_3r and class_12: {(time.time() - start):.2f} segs\")\n",
    "\n",
    "    start = time.time()  \n",
    "    out_exp = np.empty((array_mem_t.shape[0], out.shape[1]), dtype=np.float32)\n",
    "    skmap_bindings.fillArray(out_exp, n_threads, 255.)\n",
    "    skmap_bindings.expandArrayRows(out, n_threads, out_exp, selected_rows)\n",
    "    ttprint(f\"Tile {tile_id} - Reversing mask: {(time.time() - start):.2f} segs\")\n",
    "\n",
    "    start = time.time()\n",
    "    out_idx = range(0, len(model_names)+ 1)\n",
    "    out_t = np.empty((out_exp.shape[1],out_exp.shape[0]), dtype=np.float32)\n",
    "    out_gdal = np.empty((len(out_idx) * len(years),x_size *y_size), dtype=np.float32)\n",
    "    skmap_bindings.fillArray(out_gdal, n_threads, 255.)\n",
    "    skmap_bindings.transposeArray(out_exp, n_threads, out_t)\n",
    "\n",
    "    subrows = np.arange(0, len(years))\n",
    "    rows = out_idx\n",
    "    subrows_grid, rows_grid = np.meshgrid(subrows, rows)\n",
    "    inverse_idx = np.empty((out_gdal.shape[0],2), dtype=np.uintc)\n",
    "    inverse_idx[:,0] = rows_grid.flatten()\n",
    "    inverse_idx[:,1] = subrows_grid.flatten()\n",
    "\n",
    "    skmap_bindings.inverseReorderArray(out_t, n_threads, out_gdal, inverse_idx)\n",
    "    ttprint(f\"Tile {tile_id} - Transposing output: {(time.time() - start):.2f} segs\")\n",
    "\n",
    "    start = time.time()\n",
    "    write_idx = range(0, out_gdal.shape[0])\n",
    "    tmp_dir = str(make_tempdir(tile_id))\n",
    "    base_raster = landsat_files[-1]\n",
    "\n",
    "    out_files = []\n",
    "    for name in model_names: \n",
    "      out_files += [ f'gpw_eml.{name}_30m_m_{year}0101_{year}1231_go_epsg.4326_v20240206' for year in years ]\n",
    "    out_files += [ f'gpw_eml.grass.type_30m_m_{year}0101_{year}1231_go_epsg.4326_v20240206' for year in years ]\n",
    "\n",
    "    out_s3 = [ f'gaia/{s3_prefix}/{tile_id}' for o in out_files ]\n",
    "    base_raster = [ base_raster for o in out_files ]\n",
    "\n",
    "    x_off_d, y_off_d = (0, 0)\n",
    "\n",
    "    nodata_val = 255\n",
    "    compression_command = f\"gdal_translate -a_nodata {nodata_val} -co COMPRESS=deflate -co ZLEVEL=9 -co TILED=TRUE -co BLOCKXSIZE=1024 -co BLOCKYSIZE=1024\"\n",
    "\n",
    "    skmap_bindings.writeByteData(out_gdal, n_threads, gdal_opts, base_raster, tmp_dir, out_files, write_idx,\n",
    "        x_off_d, y_off_d, x_size, y_size, nodata_val, compression_command, out_s3)\n",
    "    ttprint(f\"Tile {tile_id} - Exporting output to S3: {(time.time() - start):.2f} segs\")\n",
    "\n",
    "  except:\n",
    "    tb = traceback.format_exc()\n",
    "    ttprint(f\"Tile {tile_id} - Prediction error \")\n",
    "    ttprint(tb)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac044a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
