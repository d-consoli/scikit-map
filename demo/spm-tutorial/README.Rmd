---
title: " demo: eumap Rpackage functionalities"
author: "Mohammadreza Sheykhmousa (mohammadreza.sheykhmousa@OpenGeoHub.org)"
date: "Last compiled on: `r format(Sys.time(), '%d %B, %Y')`"
output: 
   rmarkdown::html_document:
    keep_md: true
    md_extensions: +autolink_bare_uris+hard_line_breaks
    theme: united
    number_sections: false
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    toc_depth: 3
bibliography: ./tex/refs.bib
csl: ./tex/apa.csl  
fig_caption: yes
link-citations: yes
twitter-handle: opengeohub
header-includes:
- \usepackage{caption}
---

[1.1]: http://i.imgur.com/tXSoThF.png (twitter icon with padding)
[1]: https://twitter.com/sheykhmousa
Follow me on [![alt text][1.1]][1]

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = '', fig.width = 6, fig.height = 6)
```

```{r}
library(knitr)
library(eumap)
```

```{r, echo=FALSE, warning=FALSE}
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
   lines <- options$output.lines
   if (is.null(lines)) {
     return(hook_output(x, options))  # pass to default hook
   }
   x <- unlist(strsplit(x, "\n"))
   more <- "..."
   if (length(lines)==1) {        # first n lines
     if (length(x) > lines) {
       # truncate the output, but add ....
       x <- c(head(x, lines), more)
     }
   } else {
     x <- c(more, x[lines], more)
   }
   # paste these lines together
   x <- paste(c(x, ""), collapse = "\n")
   hook_output(x, options)
 })
```

## Introduction
`eumap` aims at providing easier access to EU environmental maps.
Basic functions train a spatial prediction model using [mlr3 package](https://mlr3.mlr-org.com/), [@mlr3], and related extensions in the [mlr3 ecosystem](https://github.com/mlr-org/mlr3/wiki/Extension-Packages) [@casalicchio2017openml; @MichelLang2020mlr3book], 
which includes spatial prediction using [Ensemble Machine Learning](https://koalaverse.github.io/machine-learning-in-R/stacking.html#stacking-software-in-r) taking spatial coordinates and spatial cross-validation into account. 
In a nutshell one can `train` an arbitrary `s3` **(spatial)dataframe** in `mlr3` ecosystem by defining *df* and *target.variable* i.e., response.
main functions are as the following:

## `train.spm()` 

1. `train.spm()` will automatically perform `classification` or `regression` tasks and the output is a `train.model` which later can be used to predict `newdata`.It also provides *summary* of the model and *variable importance* and *response*.
The rest of arguments can be either pass or default values will be passed. `train.spm()` provides four scenarios:

  1.1. `classification` task with **non spatial** resampling methods
  1.2. `regression` task with **non spatial** resampling methods
  1.3. `classification` task with **spatial** resampling methods
  1.4. `regression` task with **spatial** resampling methods

## `predict.spm()`

1. Prediction on a new dataset using `train.model`,
2. User needs to set`df.ts = test set` and also pass the `train.model`. 

## `accuracy.plot()` 

1. Accuracy plot in case of regression task (donâ€™t use it for classification tasks for obvious reason),
 
**Warning:** most of functions are optimized to run in parallel by default. This might result in high RAM and CPU usage.

The following examples demonstrates spatial prediction using the `meuse` data set:

## Required packages

```{r , warning=FALSE}
start_time <- Sys.time()
ls <- c("lattice", "raster", "plotKML", "ranger", "mlr3verse", "BBmisc", "knitr", "bbotk",
    "hexbin", "stringr", "magrittr", "sp", "ggplot2", "mlr3fselect", "mlr3spatiotempcv", 
    "FSelectorRcpp", "future", "future.apply", "mlr3filters", "EnvStats", "grid", "mltools","gridExtra","yardstick","plotKML", "latticeExtra","devtools")
new.packages <- ls[!(ls %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos="https://cran.rstudio.com", force=TRUE)
```

## meuse dataset

```{r ,results='hide', warning=FALSE}
library("sp")
demo(meuse, echo=FALSE)
pr.vars = c("x","y","dist","ffreq","soil","lead")
df <- as.data.frame(meuse)
df.grid <- as.data.frame(meuse.grid)
# df <- df[complete.cases(df[,pr.vars]),pr.vars]
df = na.omit(df[,])
df.grid = na.omit(df.grid[,])
summary(is.na(df))
summary(is.na(df.grid))
crs = "+init=epsg:28992"
target.variable = "lead"
```

## split training (tr) and test (ts) set  
```{r }
smp_size <- floor(0.5 * nrow(df))
set.seed(123)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
df.tr <- df[, c("x","y","dist","ffreq","soil","lead")]
df.ts <- df.grid[, c("x","y","dist","ffreq","soil")]
```

## setting generic variables 
```{r }
folds = 2
n_evals = 3
newdata = df.ts
```

## setting generic accuracy plot variables
```{r }
colorcut. = c(0,0.01,0.03,0.07,0.15,0.25,0.5,0.75,1)
colramp. = colorRampPalette(c("wheat2","red3"))
xbins. = 50
```

## Loading required libraries:
```{r, message=FALSE, warning=FALSE}
library("mlr3verse")
library("bbotk")
library("ggplot2")
library("mltools")
library("data.table")
library("mlr3fselect")
library("FSelectorRcpp")
library("future")
library("future.apply")
library("magrittr")
library("progress")
library("mlr3spatiotempcv")
library("sp")
library("landmap")  
library("dplyr")
library("EnvStats")
library("grid")
library("hexbin")
library("BBmisc")
library("lattice")
library("MASS")
library("gridExtra")
library("MLmetrics")
library("yardstick")
library("plotKML")
library("latticeExtra")
library("devtools")
library("raster")
```


```{r, echo=FALSE, warning=FALSE}
train.spm = function(df.tr, target.variable, 
parallel = TRUE, predict_type = NULL, folds = folds, method.list = NULL,  n_evals = n_evals, plot.workflow = FALSE, var.imp = TRUE, meta.learner = NULL, crs, coords = c("x","y")){
  id = deparse(substitute(df.tr))
  cv3 = rsmp("repeated_cv", folds = folds)
   if(is.factor(df.tr[,target.variable]) & missing(crs)){
    message(paste("classification Task  ","resampling method: non-spatialCV ", "ncores: ",availableCores(), "..."), immediate. = TRUE)
        message(paste0("Using learners: ", paste("method.list", collapse = ", "), "..."), immediate. = TRUE)
        tsk_clf <- mlr3::TaskClassif$new(id = id, backend = df.tr, target = target.variable)
        lrn = lrn("classif.rpart")
        gr = pipeline_robustify(tsk_clf, lrn) %>>% po("learner", lrn)
        ede = resample(tsk_clf, GraphLearner$new(gr), rsmp("holdout"))
        tsk_clasif1 = ede$task$clone()
        ranger_lrn = lrn("classif.ranger", predict_type = "response",importance ="permutation")
        ps_ranger = ParamSet$new(
           list(
             ParamInt$new("mtry", lower = 1L, upper = 5L),
             ParamDbl$new("sample.fraction", lower = 0.5, upper = 1),
             ParamInt$new("num.trees", lower = 50L, upper = 500L),
             ParamFct$new("importance", "permutation")
           ))
         
         at = AutoTuner$new(
           learner = ranger_lrn,
           resampling = cv3,
           measure = msr("classif.acc"),
           search_space = ps_ranger,
           terminator = trm("evals", n_evals = n_evals), 
           tuner = tnr("random_search")
         )
         at$store_tuning_instance = TRUE
         
        requireNamespace("lgr")
        logger = lgr::get_logger("mlr3")
        logger$set_threshold("trace")
        lgr::get_logger("mlr3")$set_threshold("warn")
        lgr::get_logger("mlr3")$set_threshold("debug")
        message("           Fitting a ensemble ML using 'mlr3::TaskClassif'...", immediate. = TRUE)
        at$train(tsk_clasif1)
        at$learner$train(tsk_clasif1)
        best.model = at$archive$best()
        var.imp = at$learner$importance()
        summary = at$learner$state$model
        tr.model = at$learner
        train.model = tr.model$predict_newdata
        response = tr.model$model$predictions
      } else if (is.numeric(df.tr[,target.variable]) & missing(crs)) {
        if( missing(predict_type)){
          predict_type <- "response" 
        }
      message(paste("Regr Task  ","resampling method: non-spatialCV ", "ncores: ",availableCores(), "..."), immediate. = TRUE)
      message(paste0("Using learners: ", paste("method.list", collapse = ", "), "..."), immediate. = TRUE)
      tsk_regr <- mlr3::TaskRegr$new(id = id, backend = df.tr, target = target.variable)
      ranger_lrn = lrn("regr.ranger", predict_type = "response",importance ="permutation")
      ps_ranger = ParamSet$new(
        list(
          ParamInt$new("mtry", lower = 1L, upper = 5L),
          ParamDbl$new("sample.fraction", lower = 0.5, upper = 1),
          ParamInt$new("num.trees", lower = 50L, upper = 500L),
          ParamFct$new("importance", "impurity")
        ))
      at = AutoTuner$new(
        learner = ranger_lrn,
        resampling = cv3,
        measure = msr("regr.rmse"),
        search_space = ps_ranger,
        terminator = trm("evals", n_evals = n_evals), 
        tuner = tnr("random_search")
      )
      at$store_tuning_instance = TRUE
      requireNamespace("lgr")
      logger = lgr::get_logger("mlr3")
      logger$set_threshold("trace")
      lgr::get_logger("mlr3")$set_threshold("warn")
      lgr::get_logger("mlr3")$set_threshold("debug")
      message("           Fitting a ensemble ML using 'mlr3::Taskregr'...", immediate. = TRUE)
      at$train(tsk_regr)
      at$learner$train(tsk_regr)
      tr.model = at$learner
      summary = tr.model$model
      var.imp = tr.model$importance()
      train.model = tr.model$predict_newdata
      response = tr.model$model$predictions
    } else if (is.factor(df.tr[,target.variable]) & crs == crs){ 
      if(is.null(method.list) & is.null(meta.learner)){
        method.list <- c("classif.ranger", "classif.rpart")
        meta.learner = "classif.ranger"}
        df.trf = mlr3::as_data_backend(df.tr)
        tsk_clf = TaskClassifST$new(id = id, backend = df.trf, target = target.variable, extra_args = list( positive = "TRUE", coordinate_names = c("x","y"), coords_as_features = FALSE,crs = crs))
        tsk_clf$missings()
        pre =  po("encode") %>>%  po("imputemode") %>>% po("removeconstants")
        g = pre %>>% gunion(list(
          po("select") %>>% po("learner_cv", id = "cv1", lrn("regr.lm")),
          po("pca") %>>% po("learner_cv", id = "cv2", lrn("regr.rpart")),
          po("nop")
        )) %>>%
          po("featureunion") %>>%
          po("learner", lrn("regr.ranger",importance ="permutation")) 
        
        g$param_set$values$cv1.resampling.method = "spcv_coords"
        g$param_set$values$cv2.resampling.method = "spcv_coords"
        if(plot.workflow == "TRUE"){
          plt = g$plot()
        }
        message(paste( "           fit the classif model  (rsmp = SPCV by cooridinates) ..."), immediate. = TRUE)
        g$train(tsk_clf)
        g$predict(tsk_clf)
        conf.mat = g$pipeops$classif.ranger$learner_model$model$confusion.matrix
        var.imp = g$pipeops$classif.ranger$learner_model$model$variable.importance
        summary = g$pipeops$classif.ranger$learner_model$model
        tr.model = g$pipeops$classif.ranger$learner$train(tsk_clf)
        train.model = tr.model$predict_newdata
        response = tr.model$model$predictions
  } else if(is.numeric(df.tr[,target.variable]) & crs == crs){
        if(is.null(method.list) & is.null(meta.learner)){
                   method.list <- c("regr.ranger", "regr.rpart")
                   meta.learner <- "regr.ranger"}
        df.trf = mlr3::as_data_backend(df.tr)
        tsk_regr = TaskRegrST$new(id = id, backend = df.trf, target = target.variable,
        extra_args = list( positive = "TRUE", coordinate_names = c("x","y"), coords_as_features = FALSE,
        crs = crs))

        tsk_regr$missings()
        pre =  po("encode") %>>%  po("imputemode") %>>% po("removeconstants")
        g = pre %>>% gunion(list(
        po("select") %>>% po("learner_cv", id = "cv1", lrn("regr.lm")),
        po("pca") %>>% po("learner_cv", id = "cv2", lrn("regr.rpart")),
        po("nop")
        )) %>>%
        po("featureunion") %>>%
        po("learner", lrn("regr.ranger",importance ="permutation")) 
        g$param_set$values$cv1.resampling.method = "spcv_coords"
        g$param_set$values$cv2.resampling.method = "spcv_coords"
        g$keep_results = "TRUE"
        if(plot.workflow == "TRUE"){
          plt = g$plot()
        }
        message(paste( "         fit the regression model  (rsmp = SPCV by cooridinates) ..."), immediate. = TRUE)
        g$train(tsk_regr)
        g$predict(tsk_regr)
        summary = g$pipeops$regr.ranger$learner_model$model
        tr.model = g$pipeops$regr.ranger$learner$train(tsk_regr)
        var.imp = tr.model$importance()
        response = tr.model$model$predictions
        train.model = tr.model$predict_newdata
        tr.model$predict_newdata
         tr.model$predict_newdata(newdata )
        }
  return(list(train.model, var.imp, summary,response))
}

```


```{r, echo=FALSE, warning=FALSE}
predict.spm = function (train.model, newdata, task = NULL){
      predict.variable = train.model(newdata)   
      y = predict.variable$response
   return(y)
}
```


```{r , echo=FALSE, warning=FALSE}

pfun <- function(x,y, ...){
  panel.xyplot(x, y, ...)
  panel.hexbinplot(x,y, ...)  
  panel.abline(coef = c(0,1), col="black", size = 0.25, lwd = 2)
}

accuracy.plot.spm <- function(x, y, main, colramp, xbins = xbins. , rng ="nat"){
if(rng == "norm"){
    x.= normalize(x, method = "range", range = c(0, 1))
    y. = normalize(y, method = "range", range = c(0, 1))
    CCC <- signif(ccc(data.frame(x,y), x, y)$.estimate, digits=3)
    plt <- hexbinplot(x. ~ y., xbins = xbins., mincnt = 1, xlab=expression(italic("0~1 measured")), ylab=expression(italic("0~1 predicted")), inner=0.2, cex.labels=1, colramp = colramp., aspect = 1, main= paste0('CCC: ', CCC), colorcut= colorcut., type="g",panel = pfun) 
  
  }
   if(rng == "nat"){
      CCC <- signif(ccc(data.frame(x,y), x, y)$.estimate, digits=3)
      plt <- hexbinplot(x ~ y, mincnt = 1, xbins=35, xlab="measured", ylab="predicted (ensemble)", inner=0.2, cex.labels=1, colramp= colramp., aspect = 1, main=paste0('CCC: ', CCC), colorcut=colorcut., type="g",panel = pfun) 
    plt  
   }
  print(plt)
  return(plt)
}
```

## `train.spm`

`train.spm` fits multiple models/learners depending on the `class()` of the **target.variable** and for returns a `trained model`, **var.imp**, **summary** of the model, and **response** variables. `trained model` later can predict a `newdata` set. 

```{r ,results='hide', warning=FALSE}
tr = train.spm(df.tr, target.variable = target.variable , folds = folds , n_evals = n_evals , crs)
```

`train.spm` results:

1st element is the *trained model*:

```{r ,results='hide', warning=FALSE}
train.model= tr[[1]]
```
2nd element is the *variable importance*:
```{r}
var.imp = tr[[2]]
var.imp
```
3rd element of the summary of the *trained model*:
```{r}
summary = tr[[3]]
summary
```

4th element is the predicted values of our trained model
note: here we just show start and the ending values
```{r ,output.lines= -(4:145)}
response = tr[[4]]
response
```

## `predict.spm()`

prediction on `newdata` set
```{r, warning=FALSE,results='hide' }
predict.variable = predict.spm(train.model, newdata)
```
### predicted values for the *newdata* set:
note: here we just show start and the ending values
```{r ,output.lines= -(3:3200)}
predict.variable
```
## `accuracy.plot.spm` 
in case of regression task,
- for now we have two scenarios including:
 - rng = "nat" provides visualizations with real values
 - rng = "norm" provides visualizations with the normalized (0~1) values

```{r, fig.align="center", fig.width=6, fig.height=6, fig.cap="Accuracy plot"}
plt = accuracy.plot.spm(x = df.tr[,target.variable], y = response, rng = "norm")
```


```{r , echo=FALSE, warning=FALSE}

df.ts$leadp = predict.variable
coordinates(df.ts) <- ~x+y
proj4string(df.ts) <- CRS("+init=epsg:28992")
# creat raster out of output
gridded(df.ts) = TRUE

```
## raster grid 
make a map using ensemble machine learning with spatial cross validation for the predicted variables e.g., *lead* (in this case) 
```{r, fig.align="center", fig.width=6, fig.height=6, fig.cap="Raster grid"}
plot(df.ts[,"leadp"])
points(meuse, pch="+")
```

## References

