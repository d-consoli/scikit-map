---
title: " Wrapper of mlr3 for (non)spatial data; Project: GeoHarmonizer_INEA"
author: "Created and maintained by: Mohammadreza Sheykhmousa (mohammadreza.sheykhmousa@OpenGeoHub.org) "
date: "Last compiled on: `r format(Sys.time(), '%d %B, %Y')`"
output: 
  md_document:
    toc: true
    toc_depth: 4
bibliography: ./tex/refs.bib
csl: ./tex/apa.csl  
fig_caption: yes
link-citations: yes
twitter-handle: opengeohub
header-includes:
- \usepackage{caption}
---

[<img src="tex/opengeohub_logo_ml.png" alt="OpenGeoHub logo" width="350"/>](https://opengeohub.org)

[1.1]: http://i.imgur.com/tXSoThF.png (twitter icon with padding)
[1]: https://twitter.com/sheykhmousa

Follow me on [![alt text][1.1]][1]

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = '', fig.width = 6, fig.height = 6)
```

### Aims of demo
- To show how to load `meuse` and `eberg` data and fit `spm` and generate `predictions`

### train.spm

`train.spm` sources `train.spm.fnc.R`, `predict.spm.fnc.R`, and `accuracy.plot.spm.fnc.R` ie., for regression tasks, functions to fulfill the aims of this tutorial using mlr3 ([Lang](https://mlr3book.mlr-org.com/introduction.html#ref-mlr3) et al. 2019)[@MichelLang2020mlr3book] package and [ecosystem](https://github.com/mlr-org/mlr3/wiki/Extension-Packages).

- In `train.spm` we need to install the required packages as followings:

```{r, warning=FALSE}
start_time <- Sys.time()
ls <- c("lattice", "raster", "plotKML", "ranger", "mlr3verse", "BBmisc", "knitr", "bbotk",
    "hexbin", "stringr", "magrittr", "sp", "ggplot2", "mlr3fselect", "mlr3spatiotempcv", 
    "FSelectorRcpp", "future", "future.apply", "mlr3filters", "EnvStats", "grid", "mltools","gridExtra","yardstick","plotKML", "latticeExtra","devtools")
new.packages <- ls[!(ls %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos="https://cran.rstudio.com", force=TRUE)
```

### meuse dataset

Below setting should works with `eberg` dataset (`crs` must change to `eberg` setting in case of SPCV resampling).

```{r include=FALSE, warning=FALSE}
library("sp")
data(meuse)
df = meuse
df <- na.omit(df)
crs = "+proj=lcc +lat_1=40.66666666666666 +lat_2=41.03333333333333 +lat_0=40.16666666666666 +lon_0=-74 +x_0=300000 +y_0=0 +datum=NAD83 +units=us-ft +no_defs"

```

Splitting training (tr) and test (ts) sets and defining generic variables
- The user can modify them.

```{r, message=FALSE, warning=FALSE}
smp_size <- floor(0.5 * nrow(df))
set.seed(123)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
df.tr <- df[train_ind, ]
df.ts <- df[-train_ind, ]
folds = 5
n_evals = 10
colorcut. = c(0,0.01,0.03,0.07,0.15,0.25,0.5,0.75,1)
colramp. = colorRampPalette(c("wheat2","red3"))
xbins. = 30
target.variable = "lead"
```


```{r, message=FALSE, warning=FALSE}

library("mlr3verse")
library("bbotk")
library("ggplot2")
library("mltools")
library("data.table")
library("mlr3fselect")
library("FSelectorRcpp")
library("future")
library("future.apply")
library("magrittr")
library("progress")
library("mlr3spatiotempcv")
library("sp")
library("landmap")  
library("dplyr")
library("EnvStats")
library("grid")
library("hexbin")
library("BBmisc")
library("lattice")
library("MASS")
library("gridExtra")
library("MLmetrics")
library("yardstick")
library("plotKML")
library("latticeExtra")
library("devtools")
```


###  `train.spm()` 

Here we have four scenarios:

- `classification` task with **non spatial** resampling methods
- `regression` task with **non spatial** resampling methods
- `classification` task with **spatial** resampling methods
- `regression` task with **spatial** resampling methods

```{r, echo=FALSE, warning=FALSE}
train.spm = function(df.tr, target.variable, 
parallel = TRUE, predict_type = NULL, folds = 5, method.list = NULL,  n_evals = 3, plot.workflow = FALSE, var.ens = TRUE, meta.learner = NULL, crs){
  id = deparse(substitute(df.tr))
  cv3 = rsmp("repeated_cv", folds = folds)
   if(is.factor(df.tr[,target.variable]) & missing(crs)){
    message(paste("classification Task  ","resampling method: non-spatialCV ", "ncores: ",availableCores(), "..."), immediate. = TRUE)
        message(paste0("Using learners: ", paste("method.list", collapse = ", "), "..."), immediate. = TRUE)
        tsk_clf <- mlr3::TaskClassif$new(id = id, backend = df.tr, target = target.variable)
        lrn = lrn("classif.rpart")
        gr = pipeline_robustify(tsk_clf, lrn) %>>% po("learner", lrn)
        ede = resample(tsk_clf, GraphLearner$new(gr), rsmp("holdout"))
        tsk_clasif1 = ede$task$clone()
        ranger_lrn = lrn("classif.ranger", predict_type = "response",importance ="permutation")
        ps_ranger = ParamSet$new(
           list(
             ParamInt$new("mtry", lower = 1L, upper = 5L),
             ParamDbl$new("sample.fraction", lower = 0.5, upper = 1),
             ParamInt$new("num.trees", lower = 50L, upper = 500L),
             ParamFct$new("importance", "permutation")
           ))
         at = AutoTuner$new(
           learner = ranger_lrn,
           resampling = cv3,
           measure = msr("classif.acc"),
           search_space = ps_ranger,
           terminator = trm("evals", n_evals = n_evals), 
           tuner = tnr("random_search")
         )
         at$store_tuning_instance = TRUE
        requireNamespace("lgr")
        logger = lgr::get_logger("mlr3")
        logger$set_threshold("trace")
        lgr::get_logger("mlr3")$set_threshold("warn")
        lgr::get_logger("mlr3")$set_threshold("debug")
        message("           Fitting a ensemble ML using 'mlr3::TaskClassif'...", immediate. = TRUE)
        at$train(tsk_clasif1)
        at$learner$train(tsk_clasif1)
        tr.mdl = at$learner
        train.model = tr.mdl$predict_newdata
      } else if (is.numeric(df.tr[,target.variable]) & missing(crs)) {
      message(paste("regression Task  ","resampling method: non-spatialCV ", "ncores: ",availableCores(), "..."), immediate. = TRUE)  
        if( missing(predict_type)){
          predict_type <- "response" 
        }
      message(paste0("Using learners: ", paste("method.list", collapse = ", "), "..."), immediate. = TRUE)
      tsk_rgr <- mlr3::TaskRegr$new(id = id, backend = df.tr, target = target.variable)
      lrn = lrn("regr.rpart")
      gr = pipeline_robustify(tsk_rgr, lrn) %>>% po("learner", lrn)
      ede = resample(tsk_rgr, GraphLearner$new(gr), rsmp("holdout"))
      tsk_regr1 = ede$task$clone()
      ranger_lrn = lrn("regr.ranger", predict_type = "response",importance ="permutation")
      
      ps_ranger = ParamSet$new(
        list(
          ParamInt$new("mtry", lower = 1L, upper = 5L),
          ParamDbl$new("sample.fraction", lower = 0.5, upper = 1),
          ParamInt$new("num.trees", lower = 50L, upper = 500L),
          ParamFct$new("importance", "impurity")
        ))
      at = AutoTuner$new(
        learner = ranger_lrn,
        resampling = cv3,
        measure = msr("regr.rmse"),
        search_space = ps_ranger,
        terminator = trm("evals", n_evals = n_evals), 
        tuner = tnr("random_search")
      )
      at$store_tuning_instance = TRUE
      
      requireNamespace("lgr")
      logger = lgr::get_logger("mlr3")
      logger$set_threshold("trace")
      lgr::get_logger("mlr3")$set_threshold("warn")
      lgr::get_logger("mlr3")$set_threshold("debug")
      message("           Fitting a ensemble ML using 'mlr3::Taskregr'...", immediate. = TRUE)
      at$train(tsk_regr1)
      
      at$learner$train(tsk_regr1)
      tr.mdl = at$learner
      train.model = tr.mdl$predict_newdata
    } else if (is.factor(df.tr[,target.variable]) & crs == crs){ 
        method.list <- c("classif.ranger", "classif.rpart")
        meta.learner = "classif.ranger"
        df.trf = mlr3::as_data_backend(df.tr)
        tsk_clf = TaskClassifST$new(id = id, backend = df.trf, target = target.variable, extra_args = list(
        positive = "TRUE", coordinate_names = c("x", "y"), coords_as_features = FALSE,crs = crs))
        
        g = gunion(list(
        po("learner_cv", id = "cv1", lrn("classif.ranger")),
        po("pca") %>>% po("learner_cv", id = "cv2", lrn("classif.rpart")),
        po("nop") %>>% po("encode") %>>%  po("imputemode") %>>% po("removeconstants")
        )) %>>%
        po("featureunion") %>>%
        po("learner", lrn("classif.ranger",importance ="permutation")) 

        g$param_set$values$cv1.resampling.method = "spcv_coords"
        g$param_set$values$cv2.resampling.method = "spcv_coords"
        if(plot.workflow == "TRUE"){
          plt = g$plot()
        }
        message(paste( "           fit the classif model  (rsmp = SPCV by cooridinates) ..."), immediate. = TRUE)
        g$train(tsk_clf)
        g$predict(tsk_clf)
        conf.mat = g$pipeops$classif.ranger$learner_model$model$confusion.matrix
        var.imp = g$pipeops$classif.ranger$learner_model$model$variable.importance
        summary = g$pipeops$classif.ranger$learner_model$model
        tr.model = g$pipeops$classif.ranger$learner$train(tsk_clf)
        train.model = tr.model$predict_newdata
        
  } else if(is.numeric(df.tr[,target.variable]) & crs == crs){
        if(is.null(method.list) & is.null(meta.learner)){
                   method.list <- c("regr.ranger", "regr.rpart")
                   meta.learner <- "regr.ranger"}
        df.trf = mlr3::as_data_backend(df.tr)
        tsk_regr = TaskRegrST$new(id = id, backend = df.trf, target = target.variable,
        extra_args = list( positive = "TRUE", coordinate_names = c("x", "y"), coords_as_features = FALSE,
        crs = crs))
        g = gunion(list(
        po("learner_cv", id = "cv1", lrn("regr.ranger")),
        po("pca") %>>% po("learner_cv", id = "cv2", lrn("regr.rpart")),
        po("nop") %>>% po("encode") %>>%  po("imputemode") %>>% po("removeconstants")
        )) %>>%
        po("featureunion") %>>%
        po("learner", lrn("regr.ranger")) 
          
        g$param_set$values$cv1.resampling.method = "spcv_coords"
        g$param_set$values$cv2.resampling.method = "spcv_coords"
        g$keep_results = "TRUE"
        if(plot.workflow == "TRUE"){
          plt = g$plot()
        }
        message(paste( "         fit the regression model  (rsmp = SPCV by cooridinates) ..."), immediate. = TRUE)
        g$train(tsk_regr)
        g$predict(tsk_regr)
        summary = g$pipeops$regr.ranger$learner_model$model
        tr.model = g$pipeops$regr.ranger$learner$train(tsk_regr)
        train.model = tr.model$predict_newdata
  }
  return(train.model)
}
```

The above code has fitted multiple models/learners depending on the `class()` of the **target.variable** and for now only returns a `trained model` function so later on we could use it to train a new dataset. 

### `predict.spm()`

prediction on new dataset

```{r, echo=FALSE, warning=FALSE}
predict.spm = function (df.ts , task = NULL){
   id = deparse(substitute(df.ts))
   if(is.factor(df.ts[,target.variable])){
       tsk_clf <- mlr3::TaskClassif$new(id = id, backend = df.ts, target = target.variable)
       predict.variable = train.model(df.ts, tsk_clf) 
       y = df.ts[,target.variable]

    } else if (is.numeric(df.ts[,target.variable])) {
        task_regr <-mlr3::TaskRegr$new(id = id, backend = df.ts, target = target.variable)
        predict.variable = train.model(df.ts, task_regr)   
        y = df.ts[,target.variable]

    } else if (is.factor(df.ts[,target.variable]) ){ 
        df.tsf = mlr3::as_data_backend(df.ts)
        tsk_clf = TaskClassifST$new(id = id, backend = df.tsf, target = target.variable, extra_args = list(
        positive = "TRUE", coordinate_names = c("x", "y"), coords_as_features = FALSE,crs = crs))
        predict.variable = train.model(df.ts, tsk_clf)   
        y = df.ts[,target.variable]

    } else if (is.numeric(df.ts[,target.variable])){
      df.tsf = mlr3::as_data_backend(df.ts)
      tsk_regr = TaskRegrST$new( id = id, backend = df.tsf, target = target.variable, extra_args = list( positive = "TRUE", coordinate_names = c("x", "y"), coords_as_features = FALSE, crs = crs))
      predict.variable = train.model(df.ts, tsk_regr)   
      y = df.ts[,target.variable]
      
    }
   return(y)
}
```

### `accuracy.plot()`

Accuracy plot in case of regression task
Note: don't use it for classification tasks for obvious reasons

```{r , echo=FALSE, warning=FALSE}

pfun <- function(x,y, ...){
  panel.xyplot(x, y, ...)
  panel.hexbinplot(x,y, ...)  
  panel.abline(coef = c(0,1), col="black", size = 0.25, lwd = 2)
}

accuracy.plot.spm <- function(x, y, main, colramp, xbins = xbins. , rng ="nat"){
if(rng == "norm"){
    x.= normalize(x, method = "range", range = c(0, 1))
    y. = normalize(y, method = "range", range = c(0, 1))
    plt <- hexbinplot(x. ~ y., xbins = xbins., mincnt = 1, xlab=expression(italic("0~1 measured")), ylab=expression(italic("0~1 predicted")), inner=0.2, cex.labels=1, colramp = colramp., aspect = 1, main= paste0('RMSE: ', '    RSQ: '), colorcut= colorcut., type="g",panel = pfun) 
  
  }
   if(rng == "nat"){
      plt <- hexbinplot(x ~ y, mincnt = 1, xbins=35, xlab="measured", ylab="predicted (ensemble)", inner=0.2, cex.labels=1, colramp= colramp., aspect = 1, main=paste0('RMSE: ', '    RSQ: '), colorcut=colorcut., type="g",panel = pfun) 
    plt  
   }
  print(plt)
  return(plt)
  }

```


### Basic requirements user needs to set
in a nutshell the user can `train` an arbitrary `s3` **(spatial) data frame** by only defining following arguments:

`train.spm()`
***

```{r ,results='hide', warning=FALSE}
train.model = train.spm(df.tr, target.variable = target.variable, folds = folds ,n_evals = n_evals, plot.workflow = TRUE, crs )
train.model
```

- User only **must** define a `df` and the `target.variable` and `train.spm()` will automatically perform `classification` or `regression` tasks.
- The rest of arguments can be set or default values will be set.
- If **crs** is set `train.spm()` will automatically take care of **spatial cross validation**.

`predict.spm()`

```{r, warning=FALSE,results='hide' }
predict.variable = predict.spm(df.ts, task = NULL)
predict.variable
```

User needs to set only `df.ts = test set` and leave the `task = NULL`

`accuracy.plot()`
***

```{r, fig.align="center", fig.width=6, fig.height=6, fig.cap="Accuracy plot"}
accuracy.plot.spm(x = df.ts[,target.variable], y = predict.variable)
```


## References
***